{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T2 - Sentiment Analysis - Extra dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rdenadai/ia369y/blob/master/notebooks/T2/T2_Sentiment_Analysis_Extra_dataset.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Sf8qSUADZB6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IA369 - Y Computação Afetiva\n",
        "\n",
        "Exercício proposto pela Prof. Paula para o alunos de sua disciplina de Pós-Graduação.\n",
        "\n",
        "### Dupla\n",
        "\n",
        "- Edgar Lopes Banhesse RA 993396\n",
        "- Rodolfo De Nadai RA 208911\n",
        "\n",
        "## T2 - Análise de Sentimentos em Textos\n",
        "\n",
        "Este notebook serve apenas para gerar e criar novo dataset baseado no Movie Reviews e SentiWordNet."
      ]
    },
    {
      "metadata": {
        "id": "UQUJOnaQWbOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "04ef75c6-cff3-4e2c-a9ab-f6c0ab4b6207"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pprint\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import nltk\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "np.warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download de alguns dataset disponibilizados pelo NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('sentence_polarity')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import sentiwordnet as wdn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.util import ngrams\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package sentence_polarity to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eNAZee9TqT_C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vamos carregar todas as frases existentes dentro do Movie Reviews vindos da biblioteca NLTK."
      ]
    },
    {
      "metadata": {
        "id": "wVLy6E09WfXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e7a858d6-08c4-4e9f-be1f-868cbf1ff840"
      },
      "cell_type": "code",
      "source": [
        "neg, pos = movie_reviews.categories()\n",
        "\n",
        "new_phrases = []\n",
        "for ids in movie_reviews.fileids(neg):\n",
        "    for phrase in movie_reviews.sents(ids)[1:]:\n",
        "        if len(phrase) > 3:\n",
        "            new_phrases.append({\n",
        "                'type': 'neg',\n",
        "                'phrase': ' '.join(phrase).lower(),\n",
        "                'over_score': 0.0\n",
        "            })\n",
        "for ids in movie_reviews.fileids(pos):\n",
        "    for phrase in movie_reviews.sents(ids):\n",
        "        if len(phrase) > 3:\n",
        "            new_phrases.append({\n",
        "                'type': 'pos',\n",
        "                'phrase': ' '.join(phrase).lower(),\n",
        "                'over_score': 0.0\n",
        "            })\n",
        "pp.pprint(new_phrases[:3])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   {'over_score': 0.0, 'phrase': 'they get into an accident .', 'type': 'neg'},\n",
            "    {   'over_score': 0.0,\n",
            "        'phrase': 'one of the guys dies , but his girlfriend continues to see '\n",
            "                  'him in her life , and has nightmares .',\n",
            "        'type': 'neg'},\n",
            "    {'over_score': 0.0, 'phrase': \"what ' s the deal ?\", 'type': 'neg'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gDJV1pxdqXEv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Como estamos usando o SentiWordNet, vamos realizar o download do dataset e importar todo ele no formato de um dicionário em python."
      ]
    },
    {
      "metadata": {
        "id": "DLfEvMfhWgCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "40e79534-3c41-4638-eb82-8392a64a8af5"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf SentiWordNet_3.0.0_20130122.txt\n",
        "!wget https://raw.githubusercontent.com/rdenadai/ia369y/master/datasets/SentiWordNet_3.0.0_20130122.txt\n",
        "!ls -lh"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "total 13M\n",
            "drwxr-xr-x 2 root root 4.0K Sep 20 00:09 sample_data\n",
            "-rw-r--r-- 1 root root  13M Sep 23 18:32 SentiWordNet_3.0.0_20130122.txt\n",
            "-rw-r--r-- 1 root root  897 Sep 23 18:32 wget-log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bVuywbILWh0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "97307b9d-096d-40d8-e491-78b61e638c17"
      },
      "cell_type": "code",
      "source": [
        "senti_word_net = {}\n",
        "with open('SentiWordNet_3.0.0_20130122.txt') as fh:\n",
        "    content = fh.readlines()\n",
        "    for line in content:\n",
        "        if not line.startswith('#'):\n",
        "            data = line.strip().split(\"\\t\")\n",
        "            if len(data) == 6:\n",
        "                pos_score = float(data[2].strip())\n",
        "                neg_score = float(data[3].strip())\n",
        "                if pos_score > 0 or neg_score > 0:\n",
        "                    pos = data[0].strip()\n",
        "                    uid = int(data[1].strip())\n",
        "                    lemmas = [lemma.name() for lemma in wn.synset_from_pos_and_offset(pos, uid).lemmas()]\n",
        "                    for lemma in lemmas:\n",
        "                        if lemma in senti_word_net:\n",
        "                            senti_word_net[lemma]['pos_score'] = pos_score if pos_score > senti_word_net[lemma]['pos_score'] else senti_word_net[lemma]['pos_score']\n",
        "                            senti_word_net[lemma]['neg_score'] = neg_score if neg_score > senti_word_net[lemma]['neg_score'] else senti_word_net[lemma]['neg_score']\n",
        "                            senti_word_net[lemma]['obj_score'] = 1 - (senti_word_net[lemma]['pos_score'] + senti_word_net[lemma]['neg_score'])\n",
        "                        else:\n",
        "                            senti_word_net[lemma] = {\n",
        "                                'pos': pos,\n",
        "                                'id': uid,\n",
        "                                'pos_score': pos_score,\n",
        "                                'neg_score': neg_score,\n",
        "                                'obj_score': 1 - (pos_score + neg_score),\n",
        "                                'SynsetTerms': [lemma.name() for lemma in wn.synset_from_pos_and_offset(pos, uid).lemmas()]\n",
        "                            }\n",
        "print('SentiWordNet size : ', len(senti_word_net))\n",
        "print('-' * 10)\n",
        "pp.pprint(next(iter(senti_word_net.items())))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentiWordNet size :  39822\n",
            "----------\n",
            "(   'able',\n",
            "    {   'SynsetTerms': ['able'],\n",
            "        'id': 1740,\n",
            "        'neg_score': 0.0,\n",
            "        'obj_score': 0.75,\n",
            "        'pos': 'a',\n",
            "        'pos_score': 0.25})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DXMPa9Dqqbe0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vamos vetorizar todas as frases que carregamos do Movie Review usando o TF-IDF para obtermos valores de cada palavra dentro de todo o contexto."
      ]
    },
    {
      "metadata": {
        "id": "tnCGsd-WWlB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "22fe0c41-048b-40f8-8742-6592adc3e997"
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n",
        "transformed_weights = vectorizer.fit_transform([phrase['phrase'] for phrase in new_phrases])\n",
        "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
        "\n",
        "tfidf_word_weights = {}\n",
        "i = 0\n",
        "for item in vectorizer.vocabulary_.items():\n",
        "    tfidf_word_weights[item[0]] = weights[item[1]]\n",
        "print('TfIdf size : ', len(tfidf_word_weights))\n",
        "print('-' * 10)\n",
        "pp.pprint(next(iter(tfidf_word_weights.items())))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TfIdf size :  956437\n",
            "----------\n",
            "('accident', 0.00022434362084179348)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UkzNoHDVqoeH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A célula abaixo irá fazer um cálculo simples e aproximado da valência das frases.\n",
        "\n",
        "Para isso vamos selecionar o score positivo / negativo de cada palavra que o SentiWordNet nos dá e somaremos para o total de palavras dentro de cada frase. Adicionamos uma correção em caso de duas palavras negativas em sequência ou duas positivas.\n",
        "\n",
        "Finalmente, realizaremos a correção para positivo ou negativo, de acordo com a classe que a frase tinha no dataset de Movie Review, adicionando o valor de peso (TF-IDF) das palavras dentro de cada frase."
      ]
    },
    {
      "metadata": {
        "id": "_YZqYo85Wmk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "1dab5b5e-3998-495f-959b-2784b587c7c4"
      },
      "cell_type": "code",
      "source": [
        "n_new_phrases = copy.deepcopy(new_phrases)\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stwords = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "for i, phrase in enumerate(n_new_phrases):\n",
        "    words = [word for word in phrase['phrase'].split() if len(word) > 1]\n",
        "    stem_words = [stemmer.stem(word) for word in words]\n",
        "    lemm_words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
        "    words = [stem if len(stem) > len(lemm_words[i]) else lemm_words[i] for i, stem in enumerate(stem_words)]\n",
        "    grams = list(ngrams(words, 2, pad_right=True))\n",
        "\n",
        "    n_grams = []\n",
        "    for gram in grams:\n",
        "        v_grams = []\n",
        "        for word in filter(None, gram):\n",
        "            word_v = senti_word_net.get(word, None)\n",
        "            pos_score = 0.0\n",
        "            neg_score = 0.0\n",
        "            if word_v:\n",
        "                pos_score = word_v.get('pos_score')\n",
        "                neg_score = word_v.get('neg_score')\n",
        "            v_grams.append((word, pos_score, neg_score))\n",
        "        n_grams.append(v_grams)\n",
        "    \n",
        "    ovr = 0.0\n",
        "    for n_gram in n_grams:\n",
        "        g1 = n_gram[0]\n",
        "        word1, pos1, neg1 = g1\n",
        "        try:\n",
        "            g2 = n_gram[1]\n",
        "            word2, pos2, neg2 = g2\n",
        "            if pos1 - neg1 >= 0 and pos2 - neg2 >= 0:\n",
        "                pos_db = 1.0\n",
        "                if pos1 > 0 and pos2 > 0:\n",
        "                    pos_db = 1.25\n",
        "                ovr += ((pos1 - neg1) + (pos2 - neg2)) * pos_db\n",
        "            elif pos1 - neg1 <= 0 and pos2 - neg2 <= 0:\n",
        "                neg_db = 1.0\n",
        "                if neg1 > 0 and neg2 > 0:\n",
        "                    neg_db = 1.25\n",
        "                ovr += ((pos1 - neg1) + (pos2 - neg2)) * neg_db\n",
        "        except IndexError:\n",
        "            pass\n",
        "\n",
        "    tfidf = 0.0\n",
        "    for word in set(words):\n",
        "        tfidf += tfidf_word_weights.get(word, 0)\n",
        "    corr = 1 + (tfidf * len(words))\n",
        "    corr = corr if n_new_phrases[i]['type'] == 'pos' else -corr\n",
        "    n_new_phrases[i]['over_score'] = corr + ovr\n",
        "\n",
        "# normalizando os valores\n",
        "scores = np.array([m['over_score'] for m in n_new_phrases])\n",
        "a, b, mmin, mmax = -100, 100, np.min(scores), np.max(scores)\n",
        "gt = np.max([np.abs(mmin), mmax])\n",
        "mmin = -gt + (-.25)\n",
        "mmax += .25\n",
        "scores = np.floor(a + (((scores - mmin) * (b-a)) / (mmax - mmin)))\n",
        "\n",
        "for i, item in enumerate(n_new_phrases):\n",
        "    n_new_phrases[i]['over_score'] = scores[i]\n",
        "\n",
        "print('-' * 20)\n",
        "print('Frases:')\n",
        "pp.pprint(n_new_phrases[:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Frases:\n",
            "[   {   'over_score': -5.0,\n",
            "        'phrase': 'they get into an accident .',\n",
            "        'type': 'neg'},\n",
            "    {   'over_score': 1.0,\n",
            "        'phrase': 'one of the guys dies , but his girlfriend continues to see '\n",
            "                  'him in her life , and has nightmares .',\n",
            "        'type': 'neg'},\n",
            "    {'over_score': -4.0, 'phrase': \"what ' s the deal ?\", 'type': 'neg'},\n",
            "    {   'over_score': -2.0,\n",
            "        'phrase': 'watch the movie and \" sorta \" find out .',\n",
            "        'type': 'neg'},\n",
            "    {   'over_score': 2.0,\n",
            "        'phrase': 'critique : a mind - fuck movie for the teen generation that '\n",
            "                  'touches on a very cool idea , but presents it in a very bad '\n",
            "                  'package .',\n",
            "        'type': 'neg'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u819VAJVXMUS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}